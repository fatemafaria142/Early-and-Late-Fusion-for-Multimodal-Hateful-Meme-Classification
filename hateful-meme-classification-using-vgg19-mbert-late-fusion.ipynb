{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1246182,"sourceType":"datasetVersion","datasetId":715500}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade scipy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-25T09:36:39.058495Z","iopub.execute_input":"2023-11-25T09:36:39.059264Z","iopub.status.idle":"2023-11-25T09:37:02.380211Z","shell.execute_reply.started":"2023-11-25T09:36:39.059229Z","shell.execute_reply":"2023-11-25T09:37:02.379068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pandas","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:37:13.854279Z","iopub.execute_input":"2023-11-25T09:37:13.854677Z","iopub.status.idle":"2023-11-25T09:37:25.127080Z","shell.execute_reply.started":"2023-11-25T09:37:13.854646Z","shell.execute_reply":"2023-11-25T09:37:25.126124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numpy","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:37:48.235687Z","iopub.execute_input":"2023-11-25T09:37:48.236580Z","iopub.status.idle":"2023-11-25T09:37:59.409887Z","shell.execute_reply.started":"2023-11-25T09:37:48.236544Z","shell.execute_reply":"2023-11-25T09:37:59.408891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install matplotlib","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:38:03.543988Z","iopub.execute_input":"2023-11-25T09:38:03.544929Z","iopub.status.idle":"2023-11-25T09:38:14.925692Z","shell.execute_reply.started":"2023-11-25T09:38:03.544883Z","shell.execute_reply":"2023-11-25T09:38:14.924551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seaborn","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:41:58.557310Z","iopub.execute_input":"2023-11-25T09:41:58.558230Z","iopub.status.idle":"2023-11-25T09:42:09.837606Z","shell.execute_reply.started":"2023-11-25T09:41:58.558194Z","shell.execute_reply":"2023-11-25T09:42:09.836465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:42:13.103004Z","iopub.execute_input":"2023-11-25T09:42:13.103373Z","iopub.status.idle":"2023-11-25T09:42:24.480674Z","shell.execute_reply.started":"2023-11-25T09:42:13.103341Z","shell.execute_reply":"2023-11-25T09:42:24.479597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\", message=\"is_categorical_dtype is deprecated\")\n\n# Ignore the FutureWarning related to use_inf_as_na in seaborn\nwarnings.filterwarnings(\"ignore\", message=\"use_inf_as_na is deprecated\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:42:38.193858Z","iopub.execute_input":"2023-11-25T09:42:38.194759Z","iopub.status.idle":"2023-11-25T09:42:38.199796Z","shell.execute_reply.started":"2023-11-25T09:42:38.194725Z","shell.execute_reply":"2023-11-25T09:42:38.198823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:42:44.030783Z","iopub.execute_input":"2023-11-25T09:42:44.031295Z","iopub.status.idle":"2023-11-25T09:42:44.035857Z","shell.execute_reply.started":"2023-11-25T09:42:44.031266Z","shell.execute_reply":"2023-11-25T09:42:44.034914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset Manipulation**","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\n# Path to your dataset directory\ndataset_dir = '/kaggle/input/facebook-hateful-meme-dataset/data'\n\n# Function to read JSONL files\ndef read_jsonl(file_path):\n    data = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            data.append(json.loads(line))\n    return data\n\n# Load train, test, dev JSONL files\ntrain_data = read_jsonl(os.path.join(dataset_dir, 'train.jsonl'))\ntest_data = read_jsonl(os.path.join(dataset_dir, 'test.jsonl'))\ndev_data = read_jsonl(os.path.join(dataset_dir, 'dev.jsonl'))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:42:47.918887Z","iopub.execute_input":"2023-11-25T09:42:47.919494Z","iopub.status.idle":"2023-11-25T09:42:48.030535Z","shell.execute_reply.started":"2023-11-25T09:42:47.919462Z","shell.execute_reply":"2023-11-25T09:42:48.029746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Images mapping to its text and label**","metadata":{}},{"cell_type":"code","source":"def map_images_to_data(data):\n    image_map = {}\n    for item in data:\n        image_id = item['id']\n        image_path = os.path.join(dataset_dir, item['img'])\n        label = item.get('label', None)  # Using dict.get() to handle missing 'label'\n        text = item.get('text', None)  # Using dict.get() to handle missing 'text'\n        image_map[image_id] = {'path': image_path, 'label': label, 'text': text}\n    return image_map\n\n# Call the function to map image data to corresponding information\ntrain_image_map = map_images_to_data(train_data)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:42:50.472070Z","iopub.execute_input":"2023-11-25T09:42:50.472411Z","iopub.status.idle":"2023-11-25T09:42:50.502940Z","shell.execute_reply.started":"2023-11-25T09:42:50.472385Z","shell.execute_reply":"2023-11-25T09:42:50.502168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **First five data entries in train dataset**","metadata":{}},{"cell_type":"code","source":"# Print information for the first five data entries in train_data\nfor idx, item in enumerate(train_data[:5]):  # Loop through the first 5 items\n    print(f\"Entry {idx + 1}:\")\n    print(f\"ID: {item['id']}\")\n    print(f\"Image: {item['img']}\")\n    print(f\"Label: {item.get('label', None)}\")  # Handling missing 'label' key\n    print(f\"Text: {item.get('text', None)}\")  # Handling missing 'text' key\n    print(\"------------\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:42:53.239107Z","iopub.execute_input":"2023-11-25T09:42:53.239454Z","iopub.status.idle":"2023-11-25T09:42:53.245623Z","shell.execute_reply.started":"2023-11-25T09:42:53.239428Z","shell.execute_reply":"2023-11-25T09:42:53.244669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **First five data entries in test dataset**","metadata":{}},{"cell_type":"code","source":"# Print information for the first five data entries in test_data\nfor idx, item in enumerate(test_data[:5]):  # Loop through the first 5 items\n    print(f\"Entry {idx + 1}:\")\n    print(f\"ID: {item['id']}\")\n    print(f\"Image: {item['img']}\")\n    print(f\"Label: {item.get('label', None)}\")  # Handling missing 'label' key\n    print(f\"Text: {item.get('text', None)}\")  # Handling missing 'text' key\n    print(\"------------\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:42:56.556804Z","iopub.execute_input":"2023-11-25T09:42:56.557163Z","iopub.status.idle":"2023-11-25T09:42:56.563055Z","shell.execute_reply.started":"2023-11-25T09:42:56.557134Z","shell.execute_reply":"2023-11-25T09:42:56.562209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **First five data entries in dev dataset**","metadata":{}},{"cell_type":"code","source":"# Print information for the first five data entries in dev_data\nfor idx, item in enumerate(dev_data[:5]):  # Loop through the first 5 items\n    print(f\"Entry {idx + 1}:\")\n    print(f\"ID: {item['id']}\")\n    print(f\"Image: {item['img']}\")\n    print(f\"Label: {item.get('label', None)}\")  # Handling missing 'label' key\n    print(f\"Text: {item.get('text', None)}\")  # Handling missing 'text' key\n    print(\"------------\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:00.195704Z","iopub.execute_input":"2023-11-25T09:43:00.196066Z","iopub.status.idle":"2023-11-25T09:43:00.202208Z","shell.execute_reply.started":"2023-11-25T09:43:00.196037Z","shell.execute_reply":"2023-11-25T09:43:00.201298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I will take only train dataset and split it into 3 parts:\n* 1st 5000 - Train \n* 2nd 1000 - Test\n* 3rd 500 - Validation","metadata":{}},{"cell_type":"code","source":"# Get the first 5000 entries for training\ntrain_subset = train_data[:5000]\n\n# Get the next 1000 entries for testing\ntest_subset = train_data[5000:6000]\n\n# Get the next 500 entries for validation\nvalidation_subset = train_data[6000:6500]\n\n# Create DataFrames for each subset\ntrain_df = pd.DataFrame([train_image_map[item['id']] for item in train_subset])\ntest_df = pd.DataFrame([train_image_map[item['id']] for item in test_subset])\nvalidation_df = pd.DataFrame([train_image_map[item['id']] for item in validation_subset])","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:02.866483Z","iopub.execute_input":"2023-11-25T09:43:02.867131Z","iopub.status.idle":"2023-11-25T09:43:02.890765Z","shell.execute_reply.started":"2023-11-25T09:43:02.867100Z","shell.execute_reply":"2023-11-25T09:43:02.889879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:05.956108Z","iopub.execute_input":"2023-11-25T09:43:05.956678Z","iopub.status.idle":"2023-11-25T09:43:06.687270Z","shell.execute_reply.started":"2023-11-25T09:43:05.956649Z","shell.execute_reply":"2023-11-25T09:43:06.686370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef preprocess_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove website links\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n\n    # Remove extra spaces\n    text = ' '.join(text.split())\n\n    # Remove emails\n    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n\n    return ' '.join(filtered_text)\n\n\n# Apply the preprocessing function to the 'text' column\ntrain_df['text'] = train_df['text'].apply(preprocess_text)\ntest_df['text'] = test_df['text'].apply(preprocess_text)\nvalidation_df['text'] = validation_df['text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:09.081624Z","iopub.execute_input":"2023-11-25T09:43:09.082451Z","iopub.status.idle":"2023-11-25T09:43:11.343800Z","shell.execute_reply.started":"2023-11-25T09:43:09.082420Z","shell.execute_reply":"2023-11-25T09:43:11.343021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train Dataset**","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:15.203554Z","iopub.execute_input":"2023-11-25T09:43:15.203938Z","iopub.status.idle":"2023-11-25T09:43:15.218944Z","shell.execute_reply.started":"2023-11-25T09:43:15.203908Z","shell.execute_reply":"2023-11-25T09:43:15.218015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get information about columns, data types, and missing values\nprint(\"\\nInformation about the train dataset:\")\nprint(train_df.info())","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:17.615271Z","iopub.execute_input":"2023-11-25T09:43:17.615615Z","iopub.status.idle":"2023-11-25T09:43:17.637942Z","shell.execute_reply.started":"2023-11-25T09:43:17.615586Z","shell.execute_reply":"2023-11-25T09:43:17.637128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image size in train dataset**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\n# Load the first image in the train_df DataFrame\nimage_path = train_df['path'].iloc[0]\nimage = Image.open(image_path)\n\n# Get the dimensions (size) of the image\nimage_size = image.size\nprint(\"Image size:\", image_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:23.227020Z","iopub.execute_input":"2023-11-25T09:43:23.227702Z","iopub.status.idle":"2023-11-25T09:43:23.247082Z","shell.execute_reply.started":"2023-11-25T09:43:23.227667Z","shell.execute_reply":"2023-11-25T09:43:23.246195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization of Label Distribution in Train Dataset**\n* # 0 - Not Hateful\n* # 1 - Hateful","metadata":{}},{"cell_type":"code","source":"label_counts = train_df['label'].value_counts()\n\n# Define custom colors for the bars ('Not hateful' and 'hateful')\ncustom_colors = ['#73aeea', '#2595b0']\n\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\n\nplt.figure(figsize=(6, 5))\n\n# Create bar plot with grid\nbars = plt.bar(label_counts.index, label_counts.values, color=custom_colors)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Set title and axis labels using custom fontdict\nplt.title('Hateful or Not Hateful meme Distribution', fontdict=font)\nplt.xlabel('Labels', fontdict=font)\nplt.ylabel('Number of Labels', fontdict=font)\n\n# Set custom font for ticks on both x and y axes\nplt.xticks(label_counts.index, label_counts.index, fontdict=font)\nplt.yticks(fontname='Serif', fontsize=10)\n\n# Adding annotations (count values) on top of each bar\nfor bar, count in zip(bars, label_counts.values):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n             ha='center', va='bottom', fontdict=font)\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:25.800458Z","iopub.execute_input":"2023-11-25T09:43:25.801331Z","iopub.status.idle":"2023-11-25T09:43:26.072101Z","shell.execute_reply.started":"2023-11-25T09:43:25.801289Z","shell.execute_reply":"2023-11-25T09:43:26.071175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization of Text Length Distribution in Train Dataset**","metadata":{}},{"cell_type":"code","source":"# Visualize text length distribution\ntext_lengths = train_df['text'].apply(lambda x: len(x.split()))\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\n# Define custom colors for the bars\ncustom_colors = ['#e34861']\nplt.figure(figsize=(8, 4))\nplt.hist(text_lengths, bins=15, color=custom_colors, alpha=0.7)\n\n# Set title and axis labels using custom fontdict\nplt.title('Text Length Distribution',fontdict=font)\nplt.xlabel('Length of Text',fontdict=font)\nplt.ylabel('Number of Texts',fontdict=font)\n\n# Set custom font for ticks on both x and y axes\nplt.xticks(fontname='Serif', fontsize=10)\nplt.yticks(fontname='Serif', fontsize=10)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:29.414472Z","iopub.execute_input":"2023-11-25T09:43:29.414820Z","iopub.status.idle":"2023-11-25T09:43:29.724777Z","shell.execute_reply.started":"2023-11-25T09:43:29.414793Z","shell.execute_reply":"2023-11-25T09:43:29.723874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Checking if there is any null values in the train dataset**","metadata":{}},{"cell_type":"code","source":"# Check for null values in  'path', 'label', and 'text' columns\nnull_image = train_df['path'].isnull().sum()\nnull_label = train_df['label'].isnull().sum()\nnull_text = train_df['text'].isnull().sum()\n\nprint(f\"Null values in 'image': {null_image}\")\nprint(f\"Null values in 'label': {null_label}\")\nprint(f\"Null values in 'text': {null_text}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:33.113180Z","iopub.execute_input":"2023-11-25T09:43:33.113942Z","iopub.status.idle":"2023-11-25T09:43:33.122038Z","shell.execute_reply.started":"2023-11-25T09:43:33.113908Z","shell.execute_reply":"2023-11-25T09:43:33.120886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Test Dataset**","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:36.356911Z","iopub.execute_input":"2023-11-25T09:43:36.357534Z","iopub.status.idle":"2023-11-25T09:43:36.367571Z","shell.execute_reply.started":"2023-11-25T09:43:36.357491Z","shell.execute_reply":"2023-11-25T09:43:36.366643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get information about columns, data types, and missing values\nprint(\"\\nInformation about the train dataset:\")\nprint(test_df.info())","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:38.624769Z","iopub.execute_input":"2023-11-25T09:43:38.625407Z","iopub.status.idle":"2023-11-25T09:43:38.635162Z","shell.execute_reply.started":"2023-11-25T09:43:38.625373Z","shell.execute_reply":"2023-11-25T09:43:38.634234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image size in test dataset**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\n# Load the first image in the test_df DataFrame\nimage_path = test_df['path'].iloc[0]\nimage = Image.open(image_path)\n\n# Get the dimensions (size) of the image\nimage_size = image.size\nprint(\"Image size:\", image_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:41.993602Z","iopub.execute_input":"2023-11-25T09:43:41.994478Z","iopub.status.idle":"2023-11-25T09:43:42.009476Z","shell.execute_reply.started":"2023-11-25T09:43:41.994434Z","shell.execute_reply":"2023-11-25T09:43:42.008546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization of Label Distribution in Test Dataset**\n* # 0 - Not Hateful\n* # 1 - Hateful","metadata":{}},{"cell_type":"code","source":"label_counts = test_df['label'].value_counts()\n\n# Define custom colors for the bars ('Not hateful' and 'Hateful')\ncustom_colors = ['#73aeea', '#2595b0']\n\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\n\nplt.figure(figsize=(6, 5))\n\n# Create bar plot with grid\nbars = plt.bar(label_counts.index, label_counts.values, color=custom_colors)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Set title and axis labels using custom fontdict\nplt.title('Hateful or Not Hateful meme Distribution', fontdict=font)\nplt.xlabel('Labels', fontdict=font)\nplt.ylabel('Number of Labels', fontdict=font)\n\n# Set custom font for ticks on both x and y axes\nplt.xticks(label_counts.index, label_counts.index, fontdict=font)\nplt.yticks(fontname='Serif', fontsize=10)\n\n# Adding annotations (count values) on top of each bar\nfor bar, count in zip(bars, label_counts.values):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n             ha='center', va='bottom', fontdict=font)\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:44.725019Z","iopub.execute_input":"2023-11-25T09:43:44.725901Z","iopub.status.idle":"2023-11-25T09:43:44.989600Z","shell.execute_reply.started":"2023-11-25T09:43:44.725867Z","shell.execute_reply":"2023-11-25T09:43:44.988725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization of Text Length Distribution in Test Dataset**","metadata":{}},{"cell_type":"code","source":"# Visualize text length distribution\ntext_lengths = test_df['text'].apply(lambda x: len(x.split()))\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\n# Define custom colors for the bars\ncustom_colors = ['#e34861']\nplt.figure(figsize=(8, 4))\nplt.hist(text_lengths, bins=15, color=custom_colors, alpha=0.7)\n\n# Set title and axis labels using custom fontdict\nplt.title('Text Length Distribution',fontdict=font)\nplt.xlabel('Length of Text',fontdict=font)\nplt.ylabel('Number of Texts',fontdict=font)\n\n# Set custom font for ticks on both x and y axes\nplt.xticks(fontname='Serif', fontsize=10)\nplt.yticks(fontname='Serif', fontsize=10)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:48.659190Z","iopub.execute_input":"2023-11-25T09:43:48.659887Z","iopub.status.idle":"2023-11-25T09:43:48.961854Z","shell.execute_reply.started":"2023-11-25T09:43:48.659854Z","shell.execute_reply":"2023-11-25T09:43:48.960876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Checking if there is any null values in the test dataset**","metadata":{}},{"cell_type":"code","source":"# Check for null values in  'path', 'label', and 'text' columns\nnull_image = test_df['path'].isnull().sum()\nnull_label = test_df['label'].isnull().sum()\nnull_text = test_df['text'].isnull().sum()\n\nprint(f\"Null values in 'image': {null_image}\")\nprint(f\"Null values in 'label': {null_label}\")\nprint(f\"Null values in 'text': {null_text}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:51.631308Z","iopub.execute_input":"2023-11-25T09:43:51.631667Z","iopub.status.idle":"2023-11-25T09:43:51.639479Z","shell.execute_reply.started":"2023-11-25T09:43:51.631637Z","shell.execute_reply":"2023-11-25T09:43:51.638476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Validation Dataset**","metadata":{}},{"cell_type":"code","source":"validation_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:53.882888Z","iopub.execute_input":"2023-11-25T09:43:53.883699Z","iopub.status.idle":"2023-11-25T09:43:53.892674Z","shell.execute_reply.started":"2023-11-25T09:43:53.883666Z","shell.execute_reply":"2023-11-25T09:43:53.891762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get information about columns, data types, and missing values\nprint(\"\\nInformation about the train dataset:\")\nprint(validation_df.info())","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:56.008788Z","iopub.execute_input":"2023-11-25T09:43:56.009523Z","iopub.status.idle":"2023-11-25T09:43:56.019338Z","shell.execute_reply.started":"2023-11-25T09:43:56.009493Z","shell.execute_reply":"2023-11-25T09:43:56.018414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image size in validation dataset**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\n# Load the first image in the validation_df DataFrame\nimage_path = validation_df['path'].iloc[0]\nimage = Image.open(image_path)\n\n# Get the dimensions (size) of the image\nimage_size = image.size\nprint(\"Image size:\", image_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:43:58.223856Z","iopub.execute_input":"2023-11-25T09:43:58.224223Z","iopub.status.idle":"2023-11-25T09:43:58.254229Z","shell.execute_reply.started":"2023-11-25T09:43:58.224195Z","shell.execute_reply":"2023-11-25T09:43:58.253306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization of Label Distribution in Validation Dataset**\n* # 0 - Not Hateful\n* # 1 - Hateful","metadata":{}},{"cell_type":"code","source":"label_counts = validation_df['label'].value_counts()\n\n# Define custom colors for the bars ('Not hateful' and 'hateful')\ncustom_colors = ['#73aeea', '#2595b0']\n\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\n\nplt.figure(figsize=(6, 5))\n\n# Create bar plot with grid\nbars = plt.bar(label_counts.index, label_counts.values, color=custom_colors)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Set title and axis labels using custom fontdict\nplt.title('Hateful or Not Hateful meme Distribution', fontdict=font)\nplt.xlabel('Labels', fontdict=font)\nplt.ylabel('Number of Labels', fontdict=font)\n\n# Set custom font for ticks on both x and y axes\nplt.xticks(label_counts.index, label_counts.index, fontdict=font)\nplt.yticks(fontname='Serif', fontsize=10)\n\n# Adding annotations (count values) on top of each bar\nfor bar, count in zip(bars, label_counts.values):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n             ha='center', va='bottom', fontdict=font)\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:44:00.673401Z","iopub.execute_input":"2023-11-25T09:44:00.674090Z","iopub.status.idle":"2023-11-25T09:44:01.011076Z","shell.execute_reply.started":"2023-11-25T09:44:00.674055Z","shell.execute_reply":"2023-11-25T09:44:01.010016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization of Text Length Distribution in Validation Dataset**","metadata":{}},{"cell_type":"code","source":"# Visualize text length distribution\ntext_lengths = validation_df['text'].apply(lambda x: len(x.split()))\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\n# Define custom colors for the bars\ncustom_colors = ['#e34861']\nplt.figure(figsize=(8, 4))\nplt.hist(text_lengths, bins=15, color=custom_colors, alpha=0.7)\n\n# Set title and axis labels using custom fontdict\nplt.title('Text Length Distribution',fontdict=font)\nplt.xlabel('Length of Text',fontdict=font)\nplt.ylabel('Number of Texts',fontdict=font)\n\n# Set custom font for ticks on both x and y axes\nplt.xticks(fontname='Serif', fontsize=10)\nplt.yticks(fontname='Serif', fontsize=10)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:44:05.419403Z","iopub.execute_input":"2023-11-25T09:44:05.420354Z","iopub.status.idle":"2023-11-25T09:44:05.728930Z","shell.execute_reply.started":"2023-11-25T09:44:05.420318Z","shell.execute_reply":"2023-11-25T09:44:05.728019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Checking if there is any null values in the validation dataset**","metadata":{}},{"cell_type":"code","source":"# Check for null values in  'path', 'label', and 'text' columns\nnull_image = test_df['path'].isnull().sum()\nnull_label = test_df['label'].isnull().sum()\nnull_text = test_df['text'].isnull().sum()\n\nprint(f\"Null values in 'image': {null_image}\")\nprint(f\"Null values in 'label': {null_label}\")\nprint(f\"Null values in 'text': {null_text}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:44:08.442868Z","iopub.execute_input":"2023-11-25T09:44:08.443226Z","iopub.status.idle":"2023-11-25T09:44:08.454148Z","shell.execute_reply.started":"2023-11-25T09:44:08.443196Z","shell.execute_reply":"2023-11-25T09:44:08.453168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchvision transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:44:11.370950Z","iopub.execute_input":"2023-11-25T09:44:11.371336Z","iopub.status.idle":"2023-11-25T09:44:22.984812Z","shell.execute_reply.started":"2023-11-25T09:44:11.371305Z","shell.execute_reply":"2023-11-25T09:44:22.983732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom transformers import BertTokenizer, BertModel\nimport torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:46:17.491908Z","iopub.execute_input":"2023-11-25T09:46:17.493037Z","iopub.status.idle":"2023-11-25T09:46:17.497911Z","shell.execute_reply.started":"2023-11-25T09:46:17.492988Z","shell.execute_reply":"2023-11-25T09:46:17.496887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Custom Multimodal Dataset**","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\n# Define your transformations using transforms.Compose\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(256),  # Crop the center to 224X224\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nclass MyMultimodalDataset(Dataset):\n    def __init__(self, image_paths, texts, labels, transform=None):\n        self.image_paths = image_paths\n        self.texts = texts\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        # Load and preprocess image\n        image = Image.open(img_path).convert('RGB')\n        if self.transform is not None:\n            image = self.transform(image)  # Apply the composed transformation\n\n        return image, text, label\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:46:20.304168Z","iopub.execute_input":"2023-11-25T09:46:20.304532Z","iopub.status.idle":"2023-11-25T09:46:20.313195Z","shell.execute_reply.started":"2023-11-25T09:46:20.304502Z","shell.execute_reply":"2023-11-25T09:46:20.312300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset and Dataloader**","metadata":{}},{"cell_type":"code","source":"# Create custom datasets with MyMultimodalDataset\ntrain_dataset = MyMultimodalDataset(train_df['path'], train_df['text'], train_df['label'], transform=transform)\ntest_dataset = MyMultimodalDataset(test_df['path'], test_df['text'], test_df['label'], transform=transform)\nval_dataset = MyMultimodalDataset(validation_df['path'], validation_df['text'], validation_df['label'], transform=transform)\n\n# Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:46:23.264885Z","iopub.execute_input":"2023-11-25T09:46:23.265611Z","iopub.status.idle":"2023-11-25T09:46:23.272230Z","shell.execute_reply.started":"2023-11-25T09:46:23.265580Z","shell.execute_reply":"2023-11-25T09:46:23.271324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **sizes of the first 10 images from the train dataset**","metadata":{}},{"cell_type":"code","source":"# Print sizes of the first 10 images from the train dataset\nprint(\"Train dataset - Image sizes:\")\nfor i in range(10):\n    img, _, _ = train_dataset[i]\n    print(f\"Sample {i + 1}: {img.size()}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:46:26.157769Z","iopub.execute_input":"2023-11-25T09:46:26.158253Z","iopub.status.idle":"2023-11-25T09:46:26.530995Z","shell.execute_reply.started":"2023-11-25T09:46:26.158216Z","shell.execute_reply":"2023-11-25T09:46:26.530133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **sizes of the first 10 images from the test dataset**","metadata":{}},{"cell_type":"code","source":"# Print sizes of the first 10 images from the test dataset\nprint(\"\\nTest dataset - Image sizes:\")\nfor i in range(10):\n    img, _, _ = test_dataset[i]\n    print(f\"Sample {i + 1}: {img.size()}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:46:29.270220Z","iopub.execute_input":"2023-11-25T09:46:29.271070Z","iopub.status.idle":"2023-11-25T09:46:29.518624Z","shell.execute_reply.started":"2023-11-25T09:46:29.271034Z","shell.execute_reply":"2023-11-25T09:46:29.517625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **sizes of the first 10 images from the validation dataset**","metadata":{}},{"cell_type":"code","source":"# Print sizes of the first 10 images from the validation dataset\nprint(\"\\nValidation dataset - Image sizes:\")\nfor i in range(10):\n    img, _, _ = val_dataset[i]\n    print(f\"Sample {i + 1}: {img.size()}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:46:31.741694Z","iopub.execute_input":"2023-11-25T09:46:31.742053Z","iopub.status.idle":"2023-11-25T09:46:31.968801Z","shell.execute_reply.started":"2023-11-25T09:46:31.742024Z","shell.execute_reply":"2023-11-25T09:46:31.967801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **VGG19 for image feature extractor**\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\n\n# Initialize vgg19_bn with IMAGENET1K_V1 weights\nvgg19_bn = models.vgg19_bn(weights='IMAGENET1K_V1', progress=True)\nvgg19_bn = torch.nn.Sequential(*(list(vgg19_bn.children())[:-1]))  # Remove the classification layer","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:47:52.110787Z","iopub.execute_input":"2023-11-25T09:47:52.111422Z","iopub.status.idle":"2023-11-25T09:47:53.736505Z","shell.execute_reply.started":"2023-11-25T09:47:52.111390Z","shell.execute_reply":"2023-11-25T09:47:53.735625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **mBERT for text feature extractor**","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel,AdamW\n# Initialize BERT tokenizer and model\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nbert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:48:09.514324Z","iopub.execute_input":"2023-11-25T09:48:09.515316Z","iopub.status.idle":"2023-11-25T09:48:11.694568Z","shell.execute_reply.started":"2023-11-25T09:48:09.515281Z","shell.execute_reply":"2023-11-25T09:48:11.693731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:48:17.533892Z","iopub.execute_input":"2023-11-25T09:48:17.534327Z","iopub.status.idle":"2023-11-25T09:48:17.540223Z","shell.execute_reply.started":"2023-11-25T09:48:17.534295Z","shell.execute_reply":"2023-11-25T09:48:17.539299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg19_bn.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:48:19.893448Z","iopub.execute_input":"2023-11-25T09:48:19.894329Z","iopub.status.idle":"2023-11-25T09:48:19.927812Z","shell.execute_reply.started":"2023-11-25T09:48:19.894283Z","shell.execute_reply":"2023-11-25T09:48:19.926865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:48:24.600449Z","iopub.execute_input":"2023-11-25T09:48:24.601170Z","iopub.status.idle":"2023-11-25T09:48:24.799090Z","shell.execute_reply.started":"2023-11-25T09:48:24.601136Z","shell.execute_reply":"2023-11-25T09:48:24.798186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport time\nfrom torch.optim import AdamW\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:48:27.622710Z","iopub.execute_input":"2023-11-25T09:48:27.623058Z","iopub.status.idle":"2023-11-25T09:48:27.627666Z","shell.execute_reply.started":"2023-11-25T09:48:27.623031Z","shell.execute_reply":"2023-11-25T09:48:27.626701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Optimizer and Loss Function**","metadata":{}},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(list(vgg19_bn.parameters()) + list(bert_model.parameters()), lr=2e-5)\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T09:48:30.506224Z","iopub.execute_input":"2023-11-25T09:48:30.506588Z","iopub.status.idle":"2023-11-25T09:48:30.513342Z","shell.execute_reply.started":"2023-11-25T09:48:30.506560Z","shell.execute_reply":"2023-11-25T09:48:30.512369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training start here**","metadata":{}},{"cell_type":"code","source":"# Set models to evaluation mode\nvgg19_bn.eval()\nbert_model.eval()\n\nnum_epochs = 1\nnum_classes = 2 \nmax_seq_length = 100  # Set your desired maximum sequence length\n\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\nstart_time = time.time()\n\n# Training loop\nfor epoch in range(num_epochs):\n    running_train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n        # Move tensors to the device\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Convert texts to tensors and pad to a fixed sequence length\n        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n\n        optimizer.zero_grad()\n\n        img_feats = vgg19_bn(images)\n        img_feats = img_feats.squeeze()\n\n        outputs = bert_model(input_ids, attention_mask=attention_mask)\n        text_feats = outputs.last_hidden_state[:, 0, :]\n\n        img_feats_reshaped = img_feats.view(img_feats.size(0), -1)  # Reshape img_feats\n\n        # Separate classifiers for image and text features\n        img_classifier = torch.nn.Sequential(\n            torch.nn.Linear(img_feats_reshaped.shape[1], 512).to(device),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(512, num_classes).to(device),\n        )\n\n        text_classifier = torch.nn.Sequential(\n            torch.nn.Linear(text_feats.shape[1], 512).to(device),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(512, num_classes).to(device),\n        )\n\n        # Get predictions for image and text modalities separately\n        img_logits = img_classifier(img_feats_reshaped)\n        text_logits = text_classifier(text_feats)\n\n        # Combine predictions using a fusion technique (e.g., simple averaging)\n        combined_logits = 0.5 * (img_logits + text_logits)  # Simple averaging\n\n        loss = criterion(combined_logits, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_train_loss += loss.item()\n        _, predicted = combined_logits.max(1)\n        total_train += labels.size(0)\n        correct_train += predicted.eq(labels).sum().item()\n\n    epoch_train_loss = running_train_loss / len(train_loader)\n    epoch_train_accuracy = correct_train / total_train\n\n    train_losses.append(epoch_train_loss)\n    train_accuracies.append(epoch_train_accuracy)\n\n    # Validation loop\n    running_val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n\n    with torch.no_grad():\n        for val_images, val_texts, val_labels in val_loader:\n            val_images = val_images.to(device)\n            val_labels = val_labels.to(device)\n\n            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n\n            val_img_feats = vgg19_bn(val_images)\n            val_img_feats = val_img_feats.squeeze()\n\n            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n            val_text_feats = val_outputs.last_hidden_state[:, 0, :]\n\n            val_img_feats_reshaped = val_img_feats.view(val_img_feats.size(0), -1)  # Reshape val_img_feats\n\n            # Separate classifiers for image and text features\n            val_img_classifier = torch.nn.Sequential(\n                torch.nn.Linear(val_img_feats_reshaped.shape[1], 512).to(device),\n                torch.nn.ReLU(),\n                torch.nn.Dropout(0.5),\n                torch.nn.Linear(512, num_classes).to(device),\n            )\n\n            val_text_classifier = torch.nn.Sequential(\n                torch.nn.Linear(val_text_feats.shape[1], 512).to(device),\n                torch.nn.ReLU(),\n                torch.nn.Dropout(0.5),\n                torch.nn.Linear(512, num_classes).to(device),\n            )\n\n            # Get predictions for image and text modalities separately\n            val_img_logits = val_img_classifier(val_img_feats_reshaped)\n            val_text_logits = val_text_classifier(val_text_feats)\n\n            # Combine predictions using a fusion technique (e.g., simple averaging)\n            val_combined_logits = 0.5 * (val_img_logits + val_text_logits)  # Simple averaging\n\n            val_loss = criterion(val_combined_logits, val_labels)\n\n            running_val_loss += val_loss.item()\n            _, val_predicted = val_combined_logits.max(1)\n            total_val += val_labels.size(0)\n            correct_val += val_predicted.eq(val_labels).sum().item()\n\n    epoch_val_loss = running_val_loss / len(val_loader)\n    epoch_val_accuracy = correct_val / total_val\n\n    val_losses.append(epoch_val_loss)\n    val_accuracies.append(epoch_val_accuracy)\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(f\"Total execution time: {execution_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:13:46.821059Z","iopub.execute_input":"2023-11-25T10:13:46.821423Z","iopub.status.idle":"2023-11-25T10:17:51.904775Z","shell.execute_reply.started":"2023-11-25T10:13:46.821396Z","shell.execute_reply":"2023-11-25T10:17:51.903680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing start here**","metadata":{}},{"cell_type":"code","source":"import time\n\n# Prepare lists to store predicted and true labels\npredicted_labels = []\ntrue_labels = []\n\n# Set models to evaluation mode\nvgg19_bn.eval()\nbert_model.eval()\n\n# Start the timer\nstart_time = time.time()\n\n# Iterate over the test_loader\nwith torch.no_grad():\n    for test_images, test_texts, test_labels in test_loader:\n        # Move tensors to the device\n        test_images = test_images.to(device)\n        test_labels = test_labels.to(device)\n\n        # Convert texts to tensors and pad to a fixed sequence length\n        test_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in test_texts]\n        test_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in test_texts], dim=0).to(device)\n        test_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in test_texts], dim=0).to(device)\n\n        # Extract image features using vgg19_bn\n        test_img_feats = vgg19_bn(test_images)\n        test_img_feats = test_img_feats.squeeze()\n\n        # Obtain textual features using mBERT\n        test_outputs = bert_model(test_input_ids, attention_mask=test_attention_mask)\n        test_text_feats = test_outputs.last_hidden_state[:, 0, :]\n\n        # Reshape image features to match textual features along the concatenation dimension\n        test_img_feats_reshaped = test_img_feats.view(test_img_feats.size(0), -1)\n\n        # Pass image features through the image classifier\n        img_logits = img_classifier(test_img_feats_reshaped)\n\n        # Pass textual features through the text classifier\n        text_logits = text_classifier(test_text_feats)\n\n        # Combine predictions using simple averaging\n        combined_logits = 0.5 * (img_logits + text_logits)  # Simple averaging\n\n        # Convert logits to predicted labels\n        batch_predicted_labels = torch.argmax(combined_logits, axis=1).cpu().numpy()\n\n        # Append the predicted labels and true labels\n        predicted_labels.extend(batch_predicted_labels)\n        true_labels.extend(test_labels.cpu().numpy().tolist())\n\n# Stop the timer\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\n\n# Print predicted and true labels\nprint(\"Predicted Labels:\", predicted_labels)\nprint(\"True Labels:\", true_labels)\nprint(f\"Total execution time for testing: {execution_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:19:16.279514Z","iopub.execute_input":"2023-11-25T10:19:16.279923Z","iopub.status.idle":"2023-11-25T10:19:40.581347Z","shell.execute_reply.started":"2023-11-25T10:19:16.279891Z","shell.execute_reply":"2023-11-25T10:19:40.580090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluation Metrics**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score, log_loss, roc_auc_score, confusion_matrix,classification_report\n\n\n\ntest_accuracy = accuracy_score(true_labels, predicted_labels)\ntest_precision = precision_score(true_labels, predicted_labels, average='macro')\ntest_recall = recall_score(true_labels, predicted_labels, average='macro')\ntest_f1 = f1_score(true_labels, predicted_labels, average='macro')\ntest_jaccard_score = jaccard_score(true_labels, predicted_labels, average='macro')\ntest_log_loss = log_loss(true_labels, predicted_labels)\ntest_roc_auc_score = roc_auc_score(true_labels, predicted_labels)\n\n\nprint(f'Test Accuracy: {test_accuracy}')\nprint(f'Test Precision: {test_precision}')\nprint(f'Test Recall: {test_recall}')\nprint(f'Test F1 Score: {test_f1}')\nprint(f'Test Jaccard Score: {test_jaccard_score}')\nprint(f'Test Log Loss: {test_log_loss}')\nprint(\"Test ROC AUC Score:\", test_roc_auc_score)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:20:08.892256Z","iopub.execute_input":"2023-11-25T10:20:08.892959Z","iopub.status.idle":"2023-11-25T10:20:08.921051Z","shell.execute_reply.started":"2023-11-25T10:20:08.892924Z","shell.execute_reply":"2023-11-25T10:20:08.920122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Classification Report**","metadata":{}},{"cell_type":"code","source":"# Generate and print the classification report\nreport = classification_report(true_labels, predicted_labels)\nprint(\"Classification Report:\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:20:12.560721Z","iopub.execute_input":"2023-11-25T10:20:12.561560Z","iopub.status.idle":"2023-11-25T10:20:12.580686Z","shell.execute_reply.started":"2023-11-25T10:20:12.561519Z","shell.execute_reply":"2023-11-25T10:20:12.579734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"# Generate confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\n# Define the custom palette\ncustom_palette = sns.color_palette(\"mako\", as_cmap=True)# Modify the number based on number of classes in the dataset\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\n\n# Create heatmap with annotations and colormap\nheatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=custom_palette, linewidths=2, linecolor='white',\n                      xticklabels=['0', '1'], yticklabels=['0', '1'],annot_kws={\"family\": \"Serif\",'weight': 'bold', 'size': 12})\n\n# Set x and y labels with the custom font dictionary\nheatmap.set_xlabel('Predicted Labels', fontdict=font)\nheatmap.set_ylabel('True Labels', fontdict=font)\nheatmap.set_title('Multimodal Hateful Meme Classification', fontdict=font)\n\n# Set font properties for tick labels on both axes\nheatmap.set_xticklabels(heatmap.get_xticklabels(), fontname='Serif', fontsize=12)\nheatmap.set_yticklabels(heatmap.get_yticklabels(), fontname='Serif', fontsize=12)\n\n# Create a color bar to indicate the scale\ncbar = heatmap.collections[0].colorbar\ncbar.set_label('Count', fontdict=font)\ncbar.ax.tick_params(labelsize=10)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:20:15.022064Z","iopub.execute_input":"2023-11-25T10:20:15.023033Z","iopub.status.idle":"2023-11-25T10:20:15.308051Z","shell.execute_reply.started":"2023-11-25T10:20:15.022998Z","shell.execute_reply":"2023-11-25T10:20:15.307174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Prediction on random (3) images from test dataset**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Set models to evaluation mode\nvgg19_bn.eval()\nbert_model.eval()\n\n# Define the number of samples to display\nnum_samples = 3\n\nfig, axes = plt.subplots(1, num_samples, figsize=(13, 5))\n\nfor i in range(num_samples):\n    # Choose a random index\n    idx = np.random.randint(len(test_loader.dataset))\n\n    # Get the sample using the index\n    image, text, label = test_loader.dataset[idx]\n\n    # Move image tensor to the device and process it\n    image = image.permute(1, 2, 0).numpy()  # Convert PyTorch tensor to numpy array\n\n    # Display the image using Matplotlib\n    axes[i].imshow(image)\n    axes[i].set_title(f\"Sample {i + 1}\\nTrue Label: {'Not Hateful' if true_labels == 0 else 'Hateful'}\\nPredicted Label: {'Not Hateful' if predicted_labels == 0 else 'Hateful'}\")  # Replace with actual class names\n    axes[i].axis('off')\n\n    # Rest of the code for processing text and predicting labels\n    # ... (Your code for text processing and label prediction)\n    \n    # Print statements for true and predicted labels\n    predicted_labels = \"Not Hateful\" if predicted_labels == 0 else \"Hateful\"  # Replace with your actual class names\n    print(f\"Sample {i + 1}: True Label - {'Not Hateful' if true_labels == 0 else 'Hateful'}, Predicted Label - {predicted_labels}\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:20:19.765568Z","iopub.execute_input":"2023-11-25T10:20:19.765951Z","iopub.status.idle":"2023-11-25T10:20:20.312543Z","shell.execute_reply.started":"2023-11-25T10:20:19.765919Z","shell.execute_reply":"2023-11-25T10:20:20.311661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Saving the model, tokenizer and classifier**","metadata":{}},{"cell_type":"code","source":"# Save the vgg19_bn model\ntorch.save(vgg19_bn.state_dict(), '/kaggle/working/multimodal_Meme_vgg19_bn_Model.pth')\n\n# Save the mBERT model\ntorch.save(bert_model.state_dict(), '/kaggle/working/multimodal_Meme_mBERT_Model.pth')\n\n# Save the tokenizer\nbert_tokenizer.save_pretrained('/kaggle/working/multimodal_Meme_mBERT_Tokenizer.json')\n\n# Save the classifier separately using torch.save\ntorch.save(classifier.state_dict(), \"/kaggle/working/multimodal_Meme_classifier.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:20:31.635447Z","iopub.execute_input":"2023-11-25T10:20:31.636322Z","iopub.status.idle":"2023-11-25T10:20:33.738093Z","shell.execute_reply.started":"2023-11-25T10:20:31.636286Z","shell.execute_reply":"2023-11-25T10:20:33.737071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the model, tokenizer and classifier**","metadata":{}},{"cell_type":"code","source":"# load the model\nvgg19_bn.load_state_dict(torch.load('/kaggle/working/multimodal_Meme_vgg19_bn_Model.pth'))\nbert_model.load_state_dict(torch.load('/kaggle/working/multimodal_Meme_mBERT_Model.pth'))\n\n\n# load the tokenizer\nbert_tokenizer.from_pretrained('/kaggle/working/multimodal_Meme_mBERT_Tokenizer.json')\n\nnum_classes = 2\n\n'''\nThis piece of code is needed before calling classifier.load_state_dict() to ensure that\nthe loaded state dictionary gets applied to the correct architecture.\n'''\nclassifier = torch.nn.Sequential(\n            torch.nn.Linear(combined_feats.shape[1], 512).to(device),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(512, num_classes).to(device),\n        )\n# load the classifier separately\nclassifier.load_state_dict(torch.load(\"/kaggle/working/multimodal_Meme_classifier.pth\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:20:48.504382Z","iopub.execute_input":"2023-11-25T10:20:48.504714Z","iopub.status.idle":"2023-11-25T10:20:49.314521Z","shell.execute_reply.started":"2023-11-25T10:20:48.504689Z","shell.execute_reply":"2023-11-25T10:20:49.313586Z"},"trusted":true},"execution_count":null,"outputs":[]}]}